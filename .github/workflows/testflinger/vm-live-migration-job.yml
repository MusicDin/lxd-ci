job_queue: $JOB_QUEUE
global_timeout: 3600
output_timeout: 1800
provision_data:
  distro: $DISTRO

test_data:
  test_cmds: |
    #!/bin/bash

    set -xeuo pipefail

    # This is used, along with DEVICE_IP, by all scriptlets to access the device
    export DEVICE_USER="ubuntu"

    # retrieve the tools installer
    curl -Ls -o install_tools.sh https://raw.githubusercontent.com/canonical/hwcert-jenkins-tools/main/install_tools.sh

    # install the scriptlets and other tools on the agent and the device, as necessary
    export TOOLS_PATH=tools
    source install_tools.sh $TOOLS_PATH

    # ensure device is available before continuing
    wait_for_ssh --allow-degraded

    # Wait for snapd to become available
    retry -- _run "timeout 5 sudo snap wait system seed.loaded"
    wait_for_snap_changes

    _run install_packages

    # LXD working
    _run sudo snap install lxd --channel=$SNAP_CHANNEL --no-wait
    wait_for_snap_changes

    _run sudo lxd init --auto
    _run sudo usermod -G lxd ubuntu

    _run snap install microceph --channel="${MICROCEPH_SNAP_CHANNEL:-latest/edge}" --cohort=+

    script=$(cat << 'END'
    #!/bin/bash
    set -ex

    # hasNeededAPIExtension: check if LXD supports the needed extension.
    hasNeededAPIExtension() (
        { set +x; } 2>/dev/null
        local needed_extension="${1}"
        lxc info | grep -qxFm1 -- "- ${needed_extension}"
    )

    # waitInstanceReady: waits for the instance to be ready (processes count > 1).
    waitInstanceReady() (
      { set +x; } 2>/dev/null
      local maxWait="${MAX_WAIT_SECONDS:-120}"
      local instName="${1}"
      local instProj="${2:-}"
      if [ -z "${instProj}" ]; then
        # Find the currently selected project.
        instProj="$(lxc project list -f csv | sed -n 's/^\([^(]\+\) (current),.*/\1/ p')"
      fi

      # Wait for the instance to report more than one process.
      processes=0
      for _ in $(seq "${maxWait}"); do
          processes="$(lxc info --project "${instProj}" "${instName}" | awk '{if ($1 == "Processes:") print $2}')"
          if [ "${processes:-0}" -ge "${MIN_PROC_COUNT:-2}" ]; then
              return 0 # Success.
          fi
          sleep 1
      done

      echo "Instance ${instName} (${instProj}) not ready after ${maxWait}s"
      return 1 # Failed.
    )

    # Disk for use by MicroCeph
    backing_file="$(mktemp)"
    truncate -s 20GiB "${backing_file}"
    CEPH_DISK="$(losetup --direct-io=on --show --find "${backing_file}")"
    export CEPH_DISK

    # Configure LXD
    lxc network create lxdbr0
    lxc profile device add default eth0 nic network=lxdbr0

    poolName="vmpool$$"
    poolDriver=zfs

    echo "==> Create storage pool using driver ${poolDriver}"
    lxc storage create "${poolName}" "${poolDriver}"
    lxc profile device add default root disk path="/" pool="${poolName}"

    # Launch two instances for our LXD cluster and wait for them to be ready.
    lxc init "${TEST_IMG:-ubuntu-minimal-daily:24.04}" member1 --vm -c limits.memory=2GiB
    lxc init "${TEST_IMG:-ubuntu-minimal-daily:24.04}" member2 --vm -c limits.memory=2GiB
    if hasNeededAPIExtension devlxd_images_vm; then
        lxc config set member1 security.devlxd.images=true
        lxc config set member2 security.devlxd.images=true
    fi
    lxc start member1
    lxc start member2
    waitInstanceReady member1
    waitInstanceReady member2
    # shellcheck disable=SC3044 # Ignore "declare is undefined" shellcheck error.
    lxc exec member1 -- sh -c "$(declare -f waitSnapdSeed); waitSnapdSeed"
    # shellcheck disable=SC3044 # Ignore "declare is undefined" shellcheck error.
    lxc exec member2 -- sh -c "$(declare -f waitSnapdSeed); waitSnapdSeed"

    for instance in member1 member2; do
        lxc exec "${instance}" -- mkdir -p /etc/systemd/system/snapd.service.d
        lxc file push - "${instance}"/etc/systemd/system/snapd.service.d/override.conf << EOF
    # Workaround for https://bugs.launchpad.net/snapd/+bug/2104066
    [Service]
    Environment=SNAPD_STANDBY_WAIT=1m
    EOF
        lxc exec "${instance}" -- systemctl daemon-reload
        lxc exec "${instance}" -- systemctl restart snapd.service
    done

    # Install LXD in the first member.
    lxc exec member1 -- snap install lxd --channel="${LXD_SNAP_CHANNEL}" --cohort=+
    lxc exec member1 -- lxd waitready --timeout=300
    if [ -n "${LXD_SIDELOAD_PATH:-}" ]; then
        lxc file push "${LXD_SIDELOAD_PATH}" member1/var/snap/lxd/common/lxd.debug
        lxc exec member1 -- systemctl restart snap.lxd.daemon
    fi

    # Initialise and configure LXD in the first member.
    lxc exec member1 -- lxd init --auto
    member1Address="$(lxc query /1.0/instances/member1?recursion=2 | jq -r ".state.network.enp5s0.addresses[0].address")"
    lxc exec member1 -- lxc config set core.https_address="${member1Address}:8443"
    lxc exec member1 -- lxc cluster enable member1
    joinToken="$(lxc exec member1 -- lxc cluster add member2 --quiet)"

    # Install LXD on the second member.
    lxc exec member2 -- snap install lxd --channel="${LXD_SNAP_CHANNEL}" --cohort=+
    lxc exec member2 -- lxd waitready --timeout=300
    if [ -n "${LXD_SIDELOAD_PATH:-}" ]; then
        lxc file push "${LXD_SIDELOAD_PATH}" member2/var/snap/lxd/common/lxd.debug
        lxc exec member2 -- systemctl restart snap.lxd.daemon
    fi

    # Create a preseed file for member2 to join member1.
    member2Address="$(lxc query /1.0/instances/member2?recursion=2 | jq -r ".state.network.enp5s0.addresses[0].address")"

    # Initialise member2 with the preseed.
    lxc exec member2 -- lxd init --preseed << EOF
    cluster:
      enabled: true
      server_address: "${member2Address}"
      cluster_token: "${joinToken}"
    EOF

    # Copy the ceph config from the local microceph snap into each cluster member.
    lxc file push -r -p /var/snap/microceph/current/conf/* member1/etc/ceph/
    lxc file push -r -p /var/snap/microceph/current/conf/* member2/etc/ceph/
    lxc exec member1 -- chmod +x /etc/ceph
    lxc exec member2 -- chmod +x /etc/ceph

    # Create the ceph storage pool
    lxc exec member1 -- lxc storage create ceph ceph --target member1
    lxc exec member1 -- lxc storage create ceph ceph --target member2
    lxc exec member1 -- lxc storage create ceph ceph

    # Create a volume in the ceph pool to test that we can live-migrate a VM with this volume attached.
    lxc exec member1 -- lxc storage volume create ceph vol1 --type=block size=64MiB

    # Create a VM in the cluster, on member1.
    lxc exec member1 -- lxc init "${TEST_IMG:-ubuntu-minimal-daily:24.04}" v1 --vm --storage ceph --target member1 -c migration.stateful=true -c limits.memory=512MiB -d root,size=3584MiB

    # Add vol1 as a disk device to the VM.
    lxc exec member1 -- lxc config device add v1 vol1-disk disk pool=ceph source=vol1

    # Start the VM.
    lxc exec member1 -- lxc start v1
    sleep 60

    # Ascertain that the VM is indeed on member1.
    [ "$(lxc exec member1 -- lxc list -c L -f csv v1)" = "member1" ]

    # Move the VM to member2 but with a timeout as this sometimes hangs indefinitely but when it works it's fast.
    lxc exec member1 -- timeout 600 lxc move v1 --target member2

    # Verify the migration did succeeded and the VM is running on member2.
    [ "$(lxc exec member1 -- lxc list -c L -f csv v1)" = "member2" ]
    END
    )

    _run "cat > test.txt" <<< "$(printf '%s\n' "$script")"
    _run chmod +x test.txt
    _run sudo ./test.txt

